{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the Semeval data and return a dataframe\n",
    "def read_semeval_data(filename):\n",
    "    '''\n",
    "    Description: reads Semantic Evaluation XML dataset and converts into a \n",
    "                 dataframffe\n",
    "    Arguments:\n",
    "                 filename: string with file path (including filename)\n",
    "    Returns :    pandas data frame\n",
    "    Important:   this function only puts positive and neutral reviews in dataframe                          \n",
    "    '''\n",
    "    f = open(filename, 'r')\n",
    "    raw_training_data = f.read()\n",
    "    xmldoc = BeautifulSoup(raw_training_data,'lxml-xml')\n",
    "    sentences = xmldoc.Reviews.find_all('sentences')\n",
    "    opinions = xmldoc.Reviews.find_all('Opinions')\n",
    "    reviews = []\n",
    "    for i in range(0,len(sentences)):\n",
    "        record = {}\n",
    "        entity_aspect_pairs = opinions[i].find_all('Opinion')\n",
    "        for ea_pair in entity_aspect_pairs:\n",
    "            ea = ea_pair.attrs['category']\n",
    "            polarity = ea_pair.attrs['polarity']\n",
    "            if(polarity == 'positive'):\n",
    "                record[ea] = 1\n",
    "            elif(polarity == 'negative'):\n",
    "                record[ea] = -1\n",
    "            else:\n",
    "                record[ea] = 0\n",
    "        \n",
    "        #test=sentences[i].get_text()\n",
    "        #r = re.compile(r'[^nmz]+')\n",
    "        #r.sub('', test)\n",
    "        #test2=re.sub(r'\\W+â+€“ ', ' ', test)\n",
    "        #print(test2)\n",
    "        #record['TEXT'] = test2\n",
    "        record['TEXT'] = sentences[i].get_text()\n",
    "        reviews.append(record)\n",
    "    #Create a dataframe\n",
    "    df=pd.DataFrame(reviews)\n",
    "    #Change order of the columns so that text appears first\n",
    "    cols = df.columns.tolist()\n",
    "    cols.sort()\n",
    "    cols.reverse()\n",
    "    df = df[cols]\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_training = read_semeval_data('data/train.xml')\n",
    "df_testing = read_semeval_data('data/test.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to clean the text data\n",
    "#Remove punctuations, newline characters and convert to lowercase.\n",
    "#Note that we are not removing dot to mark sentence boundary\n",
    "def clean_text_data(data):\n",
    "    '''\n",
    "    Description: Given text returns cleaned version\n",
    "    Arguments:\n",
    "                  data: string with raw review text\n",
    "    Returns  :\n",
    "                  cleaned: string with unwanted characters removed\n",
    "    '''\n",
    "    prog = re.compile('[\\t\\n\\r\\f\\v\\d\\']', re.UNICODE)\n",
    "    data = re.sub(prog, ' ', str(data)).lower()\n",
    "    prog = re.compile('[!\\\"#$%&\\'()*+\\,-/:;<=>?@[\\]^_`{|}~]', re.UNICODE)\n",
    "    cleaned = re.sub(prog, ' ', data)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to decode and print output labels and polarity for a review\n",
    "def output_to_labels(output):\n",
    "    '''\n",
    "     Description: Converts predicted output for a review into labels and polarity\n",
    "     Arguments: output a numpy array\n",
    "                \n",
    "    '''\n",
    "    labels = ['SERVICE#GENERAL', 'RESTAURANT#PRICES',\n",
    "       'RESTAURANT#MISCELLANEOUS', 'RESTAURANT#GENERAL', 'LOCATION#GENERAL',\n",
    "       'FOOD#STYLE_OPTIONS', 'FOOD#QUALITY', 'FOOD#PRICES',\n",
    "       'DRINKS#STYLE_OPTIONS', 'DRINKS#QUALITY', 'DRINKS#PRICES',\n",
    "       'AMBIENCE#GENERAL']\n",
    "    for index in range(len(labels)):\n",
    "        value = output[0,index]\n",
    "        if(value == 1):\n",
    "            print(labels[index],':',' ','positive')\n",
    "        elif(value == -1):\n",
    "            print(labels[index],':',' ','negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_training['TEXT'] = df_training['TEXT'].apply(clean_text_data)\n",
    "df_testing['TEXT'] = df_testing['TEXT'].apply(clean_text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to do Feature Scaling\n",
    "def standardize_features(X_train,X_test, standardize=True):\n",
    "    \"\"\"\n",
    "    Returns standardized features\n",
    "    :param X_train: Training data to be standardized\n",
    "    :param X_test : Testing data to be standardized\n",
    "    :param standardize : A flag to indicate if we need data standardized\n",
    "    :return: X_train_std,X_test_std: Standardized training and testing data\n",
    "    \"\"\"\n",
    "    standardizer = StandardScaler()\n",
    "    X_train_std = X_train\n",
    "    X_test_std = X_test\n",
    "\n",
    "    if(standardize):\n",
    "        X_train_std = standardizer.fit_transform(X_train)\n",
    "        X_test_std = standardizer.transform(X_test)\n",
    "\n",
    "    return X_train_std,X_test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prepare training and testing datasets\n",
    "X_train = df_training['TEXT']\n",
    "y_train = df_training.drop('TEXT',axis=1)\n",
    "\n",
    "X_test = df_testing['TEXT']\n",
    "y_test = df_testing.drop('TEXT',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to compute F1_Score (microaveraged)\n",
    "def compute_f1_score_micro(y_true, y_predicted):\n",
    "    '''\n",
    "    Description : Computes and returns F1 score microaveraged\n",
    "    Arguments:\n",
    "                 y_true:       True value\n",
    "                 y_ predicted: Predicted values \n",
    "    '''\n",
    "    TP,FP,TN,FN=0,0,0,0\n",
    "    TP_sum,FP_sum,TN_sum,FN_sum=0,0,0,0\n",
    "    for column_index in range(y_true.shape[1]):\n",
    "        true_values = np.array(y_true)[:,column_index]\n",
    "        predicted_values = np.array(y_predicted)[:,column_index]\n",
    "        for index in range(len(true_values)):\n",
    "            if(true_values[index]==predicted_values[index]==1):\n",
    "                TP += 1\n",
    "            elif(true_values[index]==predicted_values[index]==-1):\n",
    "                TP += 1\n",
    "            elif(true_values[index]==0 and predicted_values[index]!=0):\n",
    "                FP += 1\n",
    "            elif(true_values[index]!=0 and predicted_values[index]==0):\n",
    "                FN += 1\n",
    "            else:\n",
    "                TN += 1   \n",
    "        TP_sum = TP_sum + TP\n",
    "        FP_sum = FP_sum + FP\n",
    "        TN_sum = TN_sum + TN\n",
    "        FN_sum = FN_sum + FN\n",
    "    return ((2*TP_sum)/(FP_sum+FN_sum+(2*TP_sum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function to compute accuracy of polarity prediction\n",
    "def compute_polarity_accuracy_score(y_true, y_predicted):\n",
    "    '''\n",
    "    Description : Compute the accuracy of the polarity\n",
    "    Arguments : \n",
    "                   y_true :   True value\n",
    "                   y_ predicted: Predicted values \n",
    "    '''\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for column_index in range(y_true.shape[1]):\n",
    "        true_values = np.array(y_true)[:,column_index]\n",
    "        predicted_values = np.array(y_predicted)[:,column_index]\n",
    "        for index in range(len(true_values)):\n",
    "            if(true_values[index]==predicted_values[index]==1):\n",
    "                correct += 1\n",
    "                total += 1\n",
    "            elif(true_values[index]==predicted_values[index]==-1):\n",
    "                correct += 1\n",
    "                total += 1\n",
    "            elif(true_values[index]==1 and predicted_values[index]==-1):\n",
    "                total += 1\n",
    "            elif(true_values[index]==-1 and predicted_values[index]==1):\n",
    "                total += 1\n",
    "            else:\n",
    "                pass\n",
    "    return (correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make scoring function\n",
    "f1_scorer = make_scorer(compute_f1_score_micro, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Baseline Classifer (RandomForest with CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator Parameters are: {'clf__estimator__n_estimators': 300} 0.728430752037\n"
     ]
    }
   ],
   "source": [
    "#RandomForest Classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(max_depth=3,class_weight='balanced')))])\n",
    "#RandomForest specific parameters\n",
    "parameters = {\n",
    "'clf__estimator__n_estimators': [100,300,350]\n",
    "}\n",
    "#Find the optimal parameters for RandomForest\n",
    "model_parameter_selection = GridSearchCV(pipeline,param_grid=parameters,cv=5,scoring = f1_scorer)\n",
    "model_parameter_selection.fit(X_train, y_train)\n",
    "print(\"Best Estimator Parameters are:\",model_parameter_selection.best_params_, model_parameter_selection.best_score_)\n",
    "y_predicted = model_parameter_selection.predict(X_test)\n",
    "f1_micro=compute_f1_score_micro(y_test,y_predicted)\n",
    "polarity_accuracy = compute_polarity_accuracy_score(y_test,y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score for classifier:  0.7286890718896061\n",
      "Accuracy Score for sentiment polarity:  0.8647540983606558\n"
     ]
    }
   ],
   "source": [
    "print('F1-Score for classifier: ',f1_micro)\n",
    "print('Accuracy Score for sentiment polarity: ',polarity_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to find aspects and sentiment polarity given a review\n",
    "def analyze_review(classifier, review_text):\n",
    "    '''\n",
    "    Description : Detects Aspects and finds the polarities given review text\n",
    "    Arguments : \n",
    "                review_text : A string with review sentence/sentences\n",
    "                classifier: A model used to predict the aspects and polarities\n",
    "    '''\n",
    "    dictionary = {'review':clean_text_data(review_text)}\n",
    "    df_input=pd.DataFrame(dictionary,index=np.arange(len(dictionary.keys())))\n",
    "    output = classifier.predict(df_input['review'])\n",
    "    output_to_labels(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = model_parameter_selection\n",
    "review_text = \"great food; bad ambiance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score for classifier:  0.7286890718896061\n",
      "Accuracy Score for sentiment polarity:  0.8647540983606558\n"
     ]
    }
   ],
   "source": [
    "print('F1-Score for classifier: ',f1_micro)\n",
    "print('Accuracy Score for sentiment polarity: ',polarity_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk1=lambda x: nltk.word_tokenize(x)\n",
    "nltk2=lambda x: nltk.pos_tag(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_training['NLTK1']=df_training.TEXT.apply(nltk1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_training['NLTK2']=df_training['NLTK1'].apply(nltk2)\n",
    "df_training['NLTK3']=df_training['NLTK1'].apply(nltk2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_testing['NLTK1']=df_testing.TEXT.apply(nltk1)\n",
    "df_testing['NLTK2']=df_testing['NLTK1'].apply(nltk2)\n",
    "df_testing['NLTK3']=df_testing['NLTK1'].apply(nltk2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 16)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testing.shape\n",
    "#df_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atharp\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,335):\n",
    "    s = \"\"\n",
    "    for j in range(0,len(df_training['TEXT'][i].split())):\n",
    "        (word,pos)=df_training['NLTK2'][i][j]\n",
    "        s+=str(pos)\n",
    "        s+=str(\" \")\n",
    "        s+=str(word)\n",
    "        s+=str(\" \")\n",
    "    \n",
    "    df_training['NLTK3'][i]=s\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atharp\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,90):\n",
    "    s = \"\"\n",
    "    for j in range(0,len(df_testing['TEXT'][i].split())):\n",
    "        (word,pos)=df_testing['NLTK2'][i][j]\n",
    "        s+=str(pos)\n",
    "        s+=str(\" \")\n",
    "        s+=str(word)\n",
    "        s+=str(\" \")\n",
    "    \n",
    "    df_testing['NLTK3'][i]=s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prepare training and testing datasets\n",
    "X_train = df_training['NLTK3']\n",
    "X_test = df_testing['NLTK3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator Parameters are: {'clf__estimator__n_estimators': 350} 0.716905740249\n"
     ]
    }
   ],
   "source": [
    "#RandomForest Classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(max_depth=3,class_weight='balanced')))])\n",
    "#RandomForest specific parameters\n",
    "parameters = {\n",
    "'clf__estimator__n_estimators': [100,300,350]\n",
    "}\n",
    "#Find the optimal parameters for RandomForest\n",
    "model_parameter_selection = GridSearchCV(pipeline,param_grid=parameters,cv=5,scoring = f1_scorer)\n",
    "model_parameter_selection.fit(X_train, y_train)\n",
    "print(\"Best Estimator Parameters are:\",model_parameter_selection.best_params_, model_parameter_selection.best_score_)\n",
    "y_predicted = model_parameter_selection.predict(X_test)\n",
    "f1_micro=compute_f1_score_micro(y_test,y_predicted)\n",
    "polarity_accuracy = compute_polarity_accuracy_score(y_test,y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score for classifier:  0.7176969426467307\n",
      "Accuracy Score for sentiment polarity:  0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "print('F1-Score for classifier: ',f1_micro)\n",
    "print('Accuracy Score for sentiment polarity: ',polarity_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
